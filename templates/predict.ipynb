{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f920e689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import required packages\n",
    "from deepcell.applications import Application\n",
    "from deepcell.model_zoo.panopticnet import PanopticNet\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from cell_classification.semantic_head import create_semantic_head\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tifffile import imread, imwrite\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import find_boundaries\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4642fe2",
   "metadata": {},
   "source": [
    "## 0: Set root directory and download example dataset\n",
    "Here we are using the example data located in `/data/example_dataset/input_data`. To modify this notebook to run using your own data, simply change `base_dir` to point to your own sub-directory within the data folder. Set `base_dir`, the path to all of your imaging data (i.e. multiplexed images and segmentation masks). Subdirectory `cell_classification` will contain all of the data generated by this notebook. In the following, we expect this folder structure:\n",
    "```\n",
    "|-- base_dir\n",
    "|   |-- image_data\n",
    "|   |   |-- fov_1\n",
    "|   |   |-- fov_2\n",
    "|   |-- segmentation\n",
    "|   |   |-- deepcell_output\n",
    "|   |-- cell_classification\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974f8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the base directory\n",
    "base_dir = \"E:/angelo_lab/data/TONIC/raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2ab6c",
   "metadata": {},
   "source": [
    "## 1: set file paths and parameters\n",
    "\n",
    "### All data, images, files, etc. must be placed in the 'data' directory, and referenced via '../data/path_to_your_data'\n",
    "\n",
    "If you're interested in directly interfacing with Google Drive, consult the documentation [here](https://ark-analysis.readthedocs.io/en/latest/_rtd/google_docs_usage.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292e4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up file paths\n",
    "tiff_dir = os.path.join(base_dir, \"image_data/samples\")\n",
    "deepcell_output_dir = os.path.join(base_dir, \"segmentation_data/deepcell_output\")\n",
    "cell_classification_output_dir = os.path.join(base_dir, \"cell_classification\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89442a",
   "metadata": {},
   "source": [
    "## 2: Load data and prepare normalization dictionary\n",
    "The next step is to iterate through all the fovs and calculate the 0.999 marker expression quantile for each marker individually. This is used for normalizing the marker expressions prior to predicting marker positivity/negativity with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8256e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterate over fovs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4d193db6b6414db74f695b4069ef5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make output directory\n",
    "os.makedirs(cell_classification_output_dir, exist_ok=True)\n",
    "\n",
    "# define the channels to exclude\n",
    "exclude_channels = ['H3K9ac', 'H3K27me3', \"Au\", \"Fe\", \"Noodle\", \"Ca\"]\n",
    "\n",
    "# load data and prepare normalization dict\n",
    "fov_names = os.listdir(tiff_dir)\n",
    "# fov_names = [\"TONIC_TMA10_R1C1\", \"TONIC_TMA10_R3C6\", \"TONIC_TMA10_R5C4\"]\n",
    "fov_paths = [os.path.join(tiff_dir, fov_name) for fov_name in fov_names]\n",
    "\n",
    "# change this function to match your naming convention\n",
    "def segmentation_naming_convention(fov_path):\n",
    "    \"\"\"Prepares the path to the segmentation data for a given fov\n",
    "    Args:\n",
    "        fov_path (str): path to fov\n",
    "    Returns:\n",
    "        seg_path (str): paths to segmentation fovs\n",
    "    \"\"\"\n",
    "    fov_name = os.path.basename(fov_path)\n",
    "    return os.path.join(\n",
    "        deepcell_output_dir, fov_name + \"_feature_0.tif\"\n",
    "    )\n",
    "\n",
    "def calculate_normalization(channel_path, quantile):\n",
    "    \"\"\"Calculates the normalization value for a given channel\n",
    "    Args:\n",
    "        channel_path (str): path to channel\n",
    "        quantile (float): quantile to use for normalization\n",
    "    Returns:\n",
    "        normalization_value (float): normalization value\n",
    "    \"\"\"\n",
    "    mplex_img = imread(channel_path)\n",
    "    normalization_value = np.quantile(mplex_img, quantile)\n",
    "    chan = os.path.basename(channel_path).split(\".\")[0]\n",
    "    return chan, normalization_value\n",
    "\n",
    "def prepare_normalization_dict(\n",
    "        fov_paths, quantile=0.999, exclude_channels=[], n_subset=10, n_jobs=8\n",
    "    ):\n",
    "    \"\"\"Prepares the normalization dict for a list of fovs\n",
    "    Args:\n",
    "        fov_paths (list): list of paths to fovs\n",
    "        quantile (float): quantile to use for normalization\n",
    "        exclude_channels (list): list of channels to exclude\n",
    "        n_subset (int): number of fovs to use for normalization\n",
    "    Returns:\n",
    "        normalization_dict (dict): dict with fov names as keys and normalization values as values\n",
    "    \"\"\"\n",
    "    normalization_dict = {}\n",
    "    if n_subset is not None:\n",
    "        random.shuffle(fov_paths)\n",
    "        fov_paths = fov_paths[:n_subset]\n",
    "    print(\"Iterate over fovs...\")\n",
    "    for fov_path in tqdm(fov_paths):\n",
    "        channels = os.listdir(fov_path)\n",
    "        channels = [\n",
    "            channel for channel in channels if channel.split(\".\")[0] not in exclude_channels\n",
    "        ]\n",
    "        channel_paths = [os.path.join(fov_path, channel) for channel in channels]\n",
    "        if n_jobs > 1:\n",
    "            normalization_values = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(calculate_normalization)(channel_path, quantile)\n",
    "            for channel_path in channel_paths\n",
    "            )\n",
    "        else:\n",
    "            normalization_values = [\n",
    "                calculate_normalization(channel_path, quantile)\n",
    "                for channel_path in channel_paths\n",
    "            ]\n",
    "        for channel, normalization_value in normalization_values:\n",
    "            if channel not in normalization_dict:\n",
    "                normalization_dict[channel] = []\n",
    "            normalization_dict[channel].append(normalization_value)\n",
    "    for channel in normalization_dict.keys():\n",
    "        normalization_dict[channel] = np.mean(normalization_dict[channel])\n",
    "    return normalization_dict\n",
    "\n",
    "# Prepare or load training data normalization dict\n",
    "normalization_dict = prepare_normalization_dict(fov_paths, exclude_channels=exclude_channels)\n",
    "# normalization_dict = json.load(open(os.path.join(cell_classification_output_dir, 'normalization_dict.json')))\n",
    "\n",
    "# save normalization dict\n",
    "if not os.path.exists(os.path.join(cell_classification_output_dir, 'normalization_dict.json')):\n",
    "    with open(os.path.join(cell_classification_output_dir, 'normalization_dict.json'), 'w') as f:\n",
    "        json.dump(normalization_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e5240",
   "metadata": {},
   "source": [
    "## 3: Load model and initialize deepcell application\n",
    "The following code initializes the deepcell application and loads the model checkpoint. The checkpoint needs to be downloaded from [here](https://charitede-my.sharepoint.com/:u:/g/personal/josef-lorenz_rumberger_charite_de/Ed5iVEMreE5DqJ_WczdXS9EBFeD75ZmaLdYWXENvUvUbSg?e=r2hxK8) and put under path `checkpoints/checkpoint_125000.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f312a96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "checkpoint_path = os.path.normpath(\"../checkpoints/checkpoint_125000.h5\")\n",
    "backbone = \"efficientnetv2bs\"\n",
    "input_shape = [512,512,2]\n",
    "\n",
    "def cell_preprocess(image, **kwargs):\n",
    "    \"\"\"Preprocess input data for cell classification model.\n",
    "    Args:\n",
    "        image: array to be processed\n",
    "    Returns:\n",
    "        np.array: processed image array\n",
    "    \"\"\"\n",
    "    output = np.copy(image)\n",
    "    if len(image.shape) != 4:\n",
    "        raise ValueError(\"Image data must be 4D, got image of shape {}\".format(image.shape))\n",
    "    normalize = kwargs.get('normalize', True)\n",
    "    marker = kwargs.get('marker')\n",
    "    normalization_dict = kwargs.get('normalization_dict')\n",
    "    if normalize:\n",
    "        if marker in normalization_dict.keys():\n",
    "            print(\"Norm_factor found for marker {}\".format(marker))\n",
    "            norm_factor = normalization_dict[marker]\n",
    "        else:\n",
    "            print(\"Norm_factor not found for marker {}\".format(marker))\n",
    "            norm_factor = np.quantile(image[...,0], 0.999)\n",
    "        image[...,0] /= norm_factor\n",
    "        image = image.clip(0, 1)\n",
    "        output = np.copy(image)\n",
    "    return output\n",
    "\n",
    "def cell_postprocess(model_output):\n",
    "    return model_output\n",
    "\n",
    "def format_output(model_output):\n",
    "    return model_output[0]\n",
    "\n",
    "model = PanopticNet(\n",
    "    backbone=backbone, input_shape=input_shape,\n",
    "    norm_method=\"std\", num_semantic_classes=[1],\n",
    "    create_semantic_head=create_semantic_head, location=False,\n",
    ")\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "prep = lambda x: cell_preprocess(x, normalize=True, marker=marker, normalization_dict=normalization_dict)\n",
    "app = Application(\n",
    "    model = model,\n",
    "    model_image_shape = input_shape,\n",
    "    preprocessing_fn=cell_preprocess,\n",
    "    postprocessing_fn=cell_postprocess,\n",
    "    format_model_output_fn = format_output\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e782794",
   "metadata": {},
   "source": [
    "## 4: Make predictions with the model\n",
    "Determine if you want to (a) plot the predictions, (b) save the prediction images and (c) use test-time augmentation during inference. The script will iterate through your samples and store predictions and a file named `pred_cell_table.csv` that contains the mean-per-cell predicted marker activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76225704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot and save images\n",
    "plot_predictions = True\n",
    "save_predictions = True\n",
    "test_time_aug = True\n",
    "\n",
    "def prepare_input_data(mplex_img, instance_mask):\n",
    "    edge = find_boundaries(instance_mask, mode=\"inner\").astype(np.uint8)\n",
    "    binary_mask = np.logical_and(edge == 0, instance_mask > 0).astype(np.float32)\n",
    "    input_data = np.stack([mplex_img, binary_mask], axis=-1)[np.newaxis,...] # bhwc\n",
    "    return input_data\n",
    "\n",
    "def segment_mean(instance_mask, prediction):\n",
    "    instance_mask_flat = tf.cast(tf.reshape(instance_mask, -1), tf.int32)  # (h*w)\n",
    "    pred_flat = tf.cast(tf.reshape(prediction, -1), tf.float32)\n",
    "    sort_order = tf.argsort(instance_mask_flat)\n",
    "    instance_mask_flat = tf.gather(instance_mask_flat, sort_order)\n",
    "    uniques, _ = tf.unique(instance_mask_flat)\n",
    "    pred_flat = tf.gather(pred_flat, sort_order)\n",
    "    mean_per_cell = tf.math.segment_mean(pred_flat, instance_mask_flat)\n",
    "    mean_per_cell = tf.gather(mean_per_cell, uniques)\n",
    "    return [uniques.numpy()[1:], mean_per_cell.numpy()[1:]] # discard background\n",
    "\n",
    "def test_time_aug(input_data, channel, model, scales=[0.75,1]):\n",
    "    input_data = np.squeeze(input_data, 0)\n",
    "    h,w,_ = input_data.shape\n",
    "    tmp = []\n",
    "    for scale in scales:\n",
    "        # load img\n",
    "        img = cv2.resize(input_data[...,0], [int(h*scale), int(w*scale)])\n",
    "        binary_mask = cv2.resize(input_data[...,1], [int(h*scale), int(w*scale)], interpolation=0)\n",
    "        input_data_tmp = np.stack([img, binary_mask], axis=-1)[np.newaxis,...] # bhwc\n",
    "        seg_map = app._predict_segmentation(input_data_tmp, preprocess_kwargs={\"normalize\": True, \"marker\": channel, \"normalization_dict\": normalization_dict}, batch_size=2)\n",
    "        seg_map = np.squeeze(seg_map)\n",
    "        seg_map = cv2.resize(seg_map, (h, w))\n",
    "        tmp.append(seg_map)\n",
    "    seg_map = np.stack(tmp, -1)\n",
    "    seg_map = np.mean(seg_map, axis = -1, keepdims = True)\n",
    "    return seg_map\n",
    "\n",
    "fov_dict_list = []\n",
    "for fov_path in fov_paths:\n",
    "    out_fov_path = os.path.join(os.path.normpath(cell_classification_output_dir), os.path.basename(fov_path))\n",
    "    fov_dict = {}\n",
    "    for channel in os.listdir(fov_path):\n",
    "        channel_path = os.path.join(fov_path, channel)\n",
    "        channel = channel.split(\".\")[0]\n",
    "        if channel in exclude_channels:\n",
    "            continue\n",
    "        mplex_img = np.squeeze(imread(channel_path))\n",
    "        instance_path = segmentation_naming_convention(fov_path)\n",
    "        instance_mask = np.squeeze(imread(instance_path))\n",
    "        input_data = prepare_input_data(mplex_img, instance_mask)\n",
    "        if test_time_aug:\n",
    "            prediction = test_time_aug(input_data, channel, model)\n",
    "        else:\n",
    "            prediction = app._predict_segmentation(input_data, preprocess_kwargs={\"normalize\": True, \"marker\": channel, \"normalization_dict\": normalization_dict}, batch_size=2)\n",
    "        prediction = np.squeeze(prediction)\n",
    "        instance_mask = np.expand_dims(instance_mask, axis=-1)\n",
    "        labels, mean_per_cell = segment_mean(instance_mask, prediction)\n",
    "        if \"segmentation_label\" not in fov_dict.keys():\n",
    "            fov_dict[\"fov\"] = [os.path.basename(fov_path)]*len(labels)\n",
    "            fov_dict[\"segmentation_label\"] = labels\n",
    "        fov_dict[channel+\"_pred\"] = mean_per_cell\n",
    "        if plot_predictions:\n",
    "            fig, ax = plt.subplots(1,3, figsize=(16,16))\n",
    "            # plot stuff\n",
    "            ax[0].imshow(np.squeeze(input_data[...,0]), vmin=0, vmax=np.quantile(input_data[...,0], 0.999))\n",
    "            ax[0].set_title(channel)\n",
    "            ax[1].imshow(np.squeeze(input_data[...,1]))\n",
    "            ax[1].set_title(\"binary\")\n",
    "            ax[2].imshow(np.squeeze(prediction), vmin=0, vmax=1)\n",
    "            ax[2].set_title(channel+\"_pred\")\n",
    "            for a in ax:\n",
    "                a.set_xticks([])\n",
    "                a.set_yticks([])\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        if save_predictions:\n",
    "            os.makedirs(out_fov_path, exist_ok=True)\n",
    "            imwrite(os.path.join(out_fov_path, channel+\".tiff\"), prediction, photometric=\"minisblack\", compression=\"zlib\")\n",
    "    fov_dict_list.append(pd.DataFrame(fov_dict))\n",
    "cell_table = pd.concat(fov_dict_list, ignore_index=True)\n",
    "cell_table.to_csv(os.path.join(cell_classification_output_dir, \"pred_cell_table.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
